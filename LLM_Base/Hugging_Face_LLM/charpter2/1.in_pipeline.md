## In Pipeline

pipeline的处理流程主要分成三个，预处理-通过模型-后处理

### 预处理

预处理做三件事情，切分，编号以及填充。

切分就是把句子切成小块的单词，然后为单词编号，随后将所有句子的长度对齐，长度不足的句子用无意义字符填充。

> 这里还是cs224n的内容，只不过实现比较顶层

整合起来的代码很简单，sentiment-analysis （情绪分析）管道默认的 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english

```
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

整个流程就是拿出情感分析的管道，把原始句子输入这个管道，最后得到一组整理好的词汇向量表。


### 探索模型

我们准备好的向量词汇表，送进模型里，用 `AutoModelForSequenceClassification.from_pretrained(checkpoint)` 召唤出来一个专门做“情感分类”，把它叫做模型头。

```
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

当把 `inputs` 放进 `model` ，会输出 `outputs` 的，这里面是 `logits` ，它们是模型对每个分类最原始的打分。

### 后处理

用 `Softmax` 把`logits`变成我们能看懂的百分比概率。

```
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```

现在我们知道概率了，但是不知道什么是积极，什么是消极，这时候就需要翻开模型自带的 `model.config.id2label`。

```
model.config.id2label
```

它会告诉我们，数字0代表的是 `NEGATIVE`，数字1代表的是 `POSITIVE`。

上述一整个过程和这个是一样的

```
chinese_classifier = pipeline(
    "sentiment-analysis", 
    model="lxyuan/distilbert-base-multilingual-cased-sentiments-student"
)

print(chinese_classifier("明石是一个邪恶的港区老板娘，不过她非常的可爱"))
print(chinese_classifier(["她最讨厌的事情是赚不到什么钱", "她最喜欢的东西是红尖尖。"]))
```

