## 内部机制详解

本章的核心目标是解构 `pipeline` 的高级封装，深入理解其从原始文本到最终输出所经历的三个核心阶段：预处理、模型与后处理。掌握这些底层步骤是进行模型调试、自定义任务及后续微调工作的基础。

#### 1. 预处理 (Preprocessing)

此阶段的核心组件是 **Tokenizer**，其主要职责是将人类语言的文本输入，转换为模型能够理解的、标准化的数字张量。

其工作流程可分解为以下几个关键步骤：

* **分词 (Tokenization):**
  将原始文本字符串拆分为一个 token 列表。主流策略为“基于子词 (Subword)”的算法（如 WordPiece, BPE），它能在词汇表大小和语义保留之间取得平衡，有效处理未登录词（OOV）问题。

* **转换为ID (Conversion to IDs):**
  依据 Tokenizer 内置的词汇表，将 token 列表映射为一串唯一的整数ID，即 `input_ids`。

* **标准化批处理输入 (Standardizing Batch Inputs):**
  模型要求批处理（batch）中的所有输入具有相同的维度。为实现此目的，Tokenizer 会执行：

  * **填充 (Padding):** 在长度不足的序列末尾添加特定的 `[PAD]` token，使其长度与批次内最长的序列对齐。
  * **截断 (Truncation):** 将超出模型最大可接受长度的序列进行裁剪。
  * **注意力掩码 (Attention Mask):** 生成一个与 `input_ids` 形状相同的二进制张量。其中 `1` 代表该位置是真实的 token，`0` 代表是填充的 `[PAD]` token。此掩码用于在模型计算中指示注意力层忽略填充部分。

* **添加特殊标记 (Special Tokens):**
  根据预训练模型的具体要求，在序列的开头和/或结尾自动添加特殊标记，如 `[CLS]` 和 `[SEP]`。这些标记对于某些模型架构（如 BERT）执行特定任务（如句子分类）至关重要。

在实践中，调用 `tokenizer()` 函数并传入相应参数（如 `padding=True`, `truncation=True`, `return_tensors="pt"`），即可一步到位地完成上述所有预处理操作。

#### 2. 模型 (Model)

此阶段接收预处理好的数据，通过神经网络进行计算。

* **模型加载:**
  使用 `from_pretrained(checkpoint)` 方法加载预训练模型。`AutoModel` 类提供基础的 Transformer 结构，输出高维的隐藏状态（hidden states）；而 `AutoModelFor<Task>`（如 `AutoModelForSequenceClassification`）则在基础模型之上增加了一个针对特定任务的头部（head），可以直接输出任务相关的结果。

* **输入格式:**
  模型期望的输入是批处理的张量。即便只处理单个句子，也必须将其构造成一个批次（例如，通过 `torch.tensor([ids])` 增加一个维度）。Tokenizer 在设置 `return_tensors` 参数后会自动处理此格式。

* **模型输出 (Logits):**
  模型经过前向传播计算后，其任务头部的直接输出是 **logits**。Logits 是未经归一化的原始分数，代表了模型对每个可能类别的原始预测置信度，其值域不固定。

#### 3. 后处理 (Post-processing)

此阶段将模型的原始输出 `logits` 转换为人类易于理解的、有意义的格式。

* **概率转换:**
  应用 `Softmax` 函数于 `logits` 张量。Softmax 将原始分数转换为一个概率分布，其中所有类别的概率值都在0到1之间，且总和为1。

* **标签映射:**
  通过访问 `model.config.id2label` 这一配置属性，可以将概率最高的那个索引值，映射回其对应的具体文本标签（例如，`0` -> `"NEGATIVE"`, `1` -> `"POSITIVE"`）。

通过手动执行这三个阶段，我们能够完整复现 `pipeline` 的功能。理解这一流程是掌握 Transformers 库并将其应用于更复杂场景的基石。