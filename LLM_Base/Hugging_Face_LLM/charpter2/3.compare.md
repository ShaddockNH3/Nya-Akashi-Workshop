## 两种推理实现方式的对比分析

本笔记旨在对比和分析使用 `transformers` 库实现模型推理的两种主要方法：手动分步执行与使用高级`pipeline` API。

#### 方法一：手动分步执行 (Tokenizer + Model)

此方法将推理过程分解为独立的预处理、模型推理和后处理三个阶段，为开发者提供了最大程度的控制力和透明度。

```python
# ------------ 方法一：手动分步执行流程 ------------

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 1. 准备阶段: 加载 Tokenizer 和 Model
print("--- 方法一：手动分步执行 ---")
checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequences = ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]

# 2. 预处理 (Preprocessing)
#    将文本序列转换为模型可接受的、包含 attention_mask 的 PyTorch 张量。
print("\n[步骤 1: 预处理]")
inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
print("Tokenizer 输出:")
print(inputs)

# 3. 模型推理 (Model Inference)
#    将预处理后的张量输入模型，获取原始的 logits 输出。
print("\n[步骤 2: 模型推理]")
outputs = model(**inputs)
print("模型原始输出 (Logits):")
print(outputs.logits)

# 4. 后处理 (Post-processing)
#    将 logits 转换为概率，并映射到可读的标签。
print("\n[步骤 3: 后处理]")
# a. 应用 Softmax 函数将 logits 转换为概率
probabilities = F.softmax(outputs.logits, dim=-1)
print("转换后概率:")
print(probabilities)

# b. 提取预测结果
predictions = torch.argmax(probabilities, dim=-1)
scores = torch.max(probabilities, dim=-1)

# c. 格式化输出
results = []
for i in range(len(sequences)):
    label_name = model.config.id2label[predictions[i].item()]
    score_value = scores.values[i].item()
    results.append({'label': label_name, 'score': score_value})

print("\n[最终格式化结果]:")
print(results)
print("-" * 40)
```

#### 方法二：高级 API (`pipeline`)

此方法通过 `pipeline` 函数将完整的推理流程高度封装，旨在提供一个简单、直接的端到端解决方案。

```python
# ------------ 方法二：高级 pipeline API 流程 ------------

from transformers import pipeline

# 1. 准备阶段: 初始化 Pipeline
#    pipeline 函数在内部封装了 Tokenizer 和 Model 的加载。
print("\n--- 方法二：高级 pipeline API ---")
classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

sequences = ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]

# 2. 端到端执行 (End-to-End Execution)
#    直接输入原始文本序列，pipeline 在内部完成所有阶段并返回最终结果。
print("\n[一键执行并获得最终结果]:")
results = classifier(sequences)

print(results)
print("-" * 40)
```

#### 总结：两种方法的对比分析

| 对比维度        | 手动分步流程 (Tokenizer + Model)                            | 高级 `pipeline` API                    |
| :---------- | :---------------------------------------------------- | :----------------------------------- |
| **代码简洁度**   | 较低，需要显式编写预处理、推理和后处理的逻辑。                               | 极高，通常仅需少量代码即可完成端到端任务。                |
| **过程透明度**   | 极高，可访问并检查各阶段的中间产物（如 `input_ids`, `logits`）。           | 较低，所有中间步骤被封装，对用户不直接可见。               |
| **控制力与灵活性** | 极高，允许在流程中插入自定义逻辑，便于调试、研究和集成。                          | 较低，主要为标准化任务设计，自定义空间有限。               |
| **输出格式**    | 原始的张量（如`logits`），需要开发者手动进行后处理以获得可用结果。                 | 直接输出经过格式化的Python对象（如字典列表），易于直接解析和使用。 |
| **主要应用场景**  | 模型训练与微调、学术研究、需要访问中间层特征（如`hidden_states`）的复杂应用、底层算法调试。 | 快速原型开发、部署标准任务、对最终结果有直接需求的简单端到端推理应用。  |
