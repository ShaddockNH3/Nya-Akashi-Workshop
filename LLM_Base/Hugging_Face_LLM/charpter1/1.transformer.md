## transformer

### pipeline

[pipeline](https://huggingface.co/learn/llm-course/zh-CN/chapter1/3?fw=pt)

[colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter1/section3.ipynb)

简单来说，pipeline()是由 Hugging Face 制造的全自动工具，不需要在意里面的任何细节，只需要知道——

1. 告诉它你要做什么事情，如完形填空
2. 将句子输入进去
3. 出结果

目前总共支持以下几个模块——

- eature-extraction （获取文本的向量表示）
- fill-mask （完形填空）
- ner （命名实体识别）
- question-answering （问答）
- sentiment-analysis （情感分析）
- summarization （提取摘要）
- text-generation （文本生成）
- translation （翻译）
- zero-shot-classification （零样本分类）

colab给的demo默认是英文的，如果想使用中文，比如说

```
chinese_classifier = pipeline(
    "sentiment-analysis", 
    model="lxyuan/distilbert-base-multilingual-cased-sentiments-student"
)

print(chinese_classifier("明石是一个邪恶的港区老板娘，不过她非常的可爱"))
print(chinese_classifier(["她最讨厌的事情是赚不到什么钱", "她最喜欢的东西是红尖尖。"]))
```

使用中文模型即可。

## 分类

Transformer 可以分为三类：

- GPT-like （也被称作自回归 Transformer 模型）
- BERT-like （也被称作自动编码 Transformer 模型）
- BART/T5-like （也被称作序列到序列的 Transformer 模型）

GPT-like只能看到前面的字，然后一个一个地猜后面的字。最擅长写文章、聊天生成。

BERT-like看到整句话，然后精准地猜出被盖住的词是什么。最擅长理解句子、做分类任务。

BART/T5-like会先完整地读懂一句话（输入序列），然后再完整地生成另一句话（输出序列）。最擅长翻译、写摘要。

## 语言模型与迁移学习

什么是语言模型？

简单来说，语言模型就是将一个婴儿丢入一个拥有全世界知识的图书馆里，日复一日，年复一年，她虽然不懂每个词的真正意思，但通过看无数遍的书，它已经对“哪个词后面经常跟着哪个词”这件事了如指掌，形成了一种“语感”。

它通过无监督学习（没人教，自己看），学会了语言的统计规律。

什么是迁移学习？（hugging face核心理念）

预训练 (Pretraining): 这个过程就是语言模型训练的过程，这个过程烧钱、烧时间、烧能源，就像要从小学一路读到博士，打下广博的知识基础。

微调 (Fine-tuning): 我们把那个已经博览群书的“博士”（预训练好的模型）请过来，然后只给他看一小部分的语料，比如说明石的台词。

因为他已经懂了人类语言的规律，所以他很快就能学会明石的说话方式。这个过程，就像是让一个博士去学习一个非常具体的新技能，速度飞快，成本极低。

为什么要这样做？ 

因为从头预训练模型太难了！我们直接选用hugging face现有的预训练模型，然后稍加培训，让他为我们工作，这就是迁移学习，省钱、省力、省时间、还环保。

## tranformer 架构

编码器 (Encoder)

接收你的输入去理解它。它会把这句话的每一个词，以及词与词之间的关系，最后浓缩成一个特征表示。

解码器 (Decoder)

接收编码器的特称表示进行输出。

只有编码器 (Encoder-only): 比如 BERT。因为它只需要理解，所以最擅长做判断题，比如情感分类、命名实体识别。

只有解码器 (Decoder-only): 比如 GPT。因为它只需要创造，所以最擅长做续写题，比如文本生成、聊天。

编码器+解码器 (Encoder-Decoder): 比如 T5。因为它既要理解输入，又要创造输出，所以最擅长做翻译题，比如中译英、文章摘要。

## 注意力机制

注意力层在处理每个词的时候，它都会自动地去照射句子里的其他词，判断哪些词对理解当前这个词最重要，然后给它们更高的关注权重。

有了它之后模型才能理解长距离的依赖关系，抓住句子的真正重点，而不是像个金鱼一样只有七秒记忆。

Attention is ALL you need!

## 微调

架构 (Architecture): 这是模型的设计，比如 BERT 架构，它规定了模型有几层、每层有什么、注意力层怎么放等等。模型蓝图本身是空的。

权重参数/检查点 (Checkpoints/Weights): 这是按照蓝图，用无数的数据训练出来的。

比如 bert-base-cased，它就是 Google 用海量英文数据，按照 BERT 蓝图，训练出来的模型，里面权重参数都充满了知识。

我们做的微调，其实就是：

我们不重新设计架构，也不重新训练一整个模型，而是直接使用bert-base-cased这座现成的模型，然后只对里面的一小部分权重进行一些调整即可。