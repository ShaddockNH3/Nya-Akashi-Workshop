## Decoder

Decoder只负责生成文本，而不负责深度理解输入。这部分只保留了Transformer的右侧。

最擅长的是文本生成任务，例如对话系统、文章续写、诗歌创作等。

Decoder在生成文本时，是逐词进行的。在预测下一个词时，它的注意力层只能访问到已经生成出的、位于当前位置前面的词，无法看到未来的信息。这个过程被称为“自回归（Auto-Regressive）”，其注意力机制是“单向的（unidirectional）”。

训练Decoder的方式是因果语言建模（Causal Language Modeling, CLM），通俗地讲就是“预测下一个词”。模型会学习在给定一段文本的前提下，预测接下来最可能出现的词是什么。通过在海量文本上进行这种训练，模型便掌握了语言的流畅性和生成能力。

Decoder家族

* GPT & GPT-2: 生成式模型的先驱，证明了自回归架构的巨大潜力，是现代大型对话模型（如ChatGPT）的直系祖先。
* Transformer-XL: 解决了原始模型在处理长文本时“遗忘”早期内容的缺陷，增强了长距离依赖的建模能力。
* CTRL: 引入了控制码（Control Codes）的概念，允许用户在生成文本时指定风格、主题等属性，实现了可控的文本生成。