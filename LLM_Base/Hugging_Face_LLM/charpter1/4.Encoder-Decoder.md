## Encoder-Decoder

Encoder-Decoder模型完整地使用了Transformer的原始架构，同时包含左侧的Encoder和右侧的Decoder。它的工作模式是“先理解，再生成”。

最擅长处理那些“输入序列”和“输出序列”不尽相同的任务，被称为序列到序列（Sequence-to-Sequence, Seq2Seq）任务。典型应用包括：

* 翻译: 将一种语言的句子（输入序列）翻译成另一种语言（输出序列）。
* 摘要生成: 将一篇长文章（输入序列）总结成几句核心摘要（输出序列）。
* 问答（生成式）: 根据输入的问题，生成一段自然语言作为答案，而不是从原文中抽取。

**工作原理**
该模型的工作分为两步：

1. **编码阶段**: Encoder首先读取并完整地理解整个输入序列，将其压缩成一个包含丰富上下文信息的中间表示（思想精华）。
2. **解码阶段**: Decoder接收这个中间表示，并在此基础上，以自回归的方式逐词生成输出序列。在生成过程中，Decoder不仅会关注自己已经生成的部分，还会持续地关注Encoder对原始输入的完整理解，确保生成内容与输入高度相关。

**预训练方式**
其预训练方式通常也比较复杂，会破坏输入序列（比如打乱句子顺序、遮盖一部分文本片段），然后要求模型恢复出原始的、完整的文本。

**Encoder-Decoder家族代表**

* **BART**: 采用了一种“去噪自编码器”的预训练方式，擅长修复损坏的文本，因此在文本生成和摘要任务上表现优异。
* **T5**: 提出了一个“万物皆可Seq2Seq”的统一框架，将所有NLP任务（如分类、问答）都转换成“文本到文本”的生成格式，模型通用性极强。
* **MARIAN**: 专门为机器翻译任务高度优化的模型，通常在翻译领域表现出色。
