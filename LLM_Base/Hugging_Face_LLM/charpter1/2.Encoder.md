## encoder

encoder只负责理解句子，而不负责输出。这部分只保留了transformer的左侧。

比较擅长的是分类，命名实体识别和问答。

分类，比如说给出一个句子，只需要分析它到底是正面情感还是负面情感。

命名实体识别就是给出一个句子，提取其中的名词之类的。

问答就是给出一段语料，然后根据需求提取核心内容。

encoder的注意力层都能访问整个句子的所有单词，可以从前到后，再从后到前，反复地审视每一个词和它所有上下文的关系，这就是双向注意力机制。

反例就是gpt的生成，gpt生成的时候不可能预先的知道自己后面要输出的是什么，因此只有单向的记忆。

训练encoder的方式是自编码预训练，简单来说就是给出语料，然后随机遮盖单词(Masked Language Modeling, MLM)，进行很多次的完形填空训练最终得出模型。

encoder家族

- ERT: 开创了双向理解的先河。
- DistilBERT: 轻量级，速度快、占内存小，是 BERT 的精华提炼版。
- RoBERTa: BERT 的超级进化体，训练得更久、更充分，通常比原版 BERT 更强。
- ALBERT: 节能，通过一些巧妙的设计，在保持高性能的同时，大大减少了参数量。
- ELECTRA: 训练方法最独特的革新者，它不是猜被遮住的词，而是判断句子里的某个词是不是被换掉了，训练效率极高。