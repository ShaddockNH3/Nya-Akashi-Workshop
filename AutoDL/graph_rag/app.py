import streamlit as st
import uuid
import sys
import os

# --- ã€åˆ›ä¸–çºªÂ·ç¬¬ä¸€æ³•åˆ™ã€‘(ä¸å˜) ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from graph_rag.engine import AkashiGraphRAGEngine
from langchain_core.messages import HumanMessage, AIMessage

st.set_page_config(page_title="Akashi RAG", page_icon="ğŸ±")
st.title("æ˜çŸ³ RAG é—®ç­”ç³»ç»Ÿ V6.9 ğŸ±")

@st.cache_resource
def load_rag_engine():
    return AkashiGraphRAGEngine()

rag_engine = load_rag_engine()
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": st.session_state.session_id}}
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- å†å²æ¶ˆæ¯æ¸²æŸ“ (ä¸å˜) ---
for msg in st.session_state.messages:
    role = "user" if msg.type == "human" else "assistant"
    with st.chat_message(role):
        st.markdown(msg.content)
        if hasattr(msg, 'additional_kwargs') and 'detail' in msg.additional_kwargs:
            detail = msg.additional_kwargs['detail']
            with st.expander("ğŸ”¬ ç‚¹å‡»å±•å¼€ï¼Œå›é¡¾æœ¬æ¬¡å›ç­”çš„è¯Šæ–­ä¿¡æ¯"):
                st.markdown(f"**1. æ”¹å†™åçš„ç‹¬ç«‹é—®é¢˜**:\n\n`{detail['standalone_question']}`")
                st.markdown("**2. æ£€ç´¢åˆ°çš„æ ¸å¿ƒæƒ…æŠ¥**:")
                if not detail["documents"]:
                    st.markdown("* (æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£)*")
                else:
                    for doc in detail["documents"]:
                        doc_type = doc.metadata.get("type", "Story Snippet")
                        st.markdown(f"* **[{doc_type}]** `{doc.metadata.get('source', 'N/A')}`")
                st.markdown(f"**3. æç‚¼çš„æ ¸å¿ƒäº‹å®**:\n\n```\n{detail['synthesized_facts']}\n```")

if prompt := st.chat_input("æŒ‡æŒ¥å®˜ï¼Œæœ‰ä»€ä¹ˆæƒ³é—®æ˜çŸ³çš„å–µï¼Ÿ"):
    human_message = HumanMessage(content=prompt)
    st.session_state.messages.append(human_message)
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.expander("ğŸ”¬ **æ­£åœ¨æ€è€ƒä¸­... ç‚¹å‡»å±•å¼€æŸ¥çœ‹å®æ—¶å·¥ä½œæµ**", expanded=True):
            log_placeholder = st.empty()
            log_content = ""
        answer_placeholder = st.empty()
        
        final_state = {}
        inputs = {"messages": st.session_state.messages}
        
        # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘---
        # æˆ‘ä»¬å°†å¾ªç¯å˜é‡å‘½åä¸º node_stateï¼Œå› ä¸ºå®ƒå°±æ˜¯èŠ‚ç‚¹çš„çŠ¶æ€å­—å…¸
        for node_state in rag_engine.graph.stream(inputs, config=config, stream_mode="values"):
            # 1. ç›´æ¥æ£€æŸ¥ 'workflow_log' æ˜¯å¦åœ¨çŠ¶æ€å­—å…¸ä¸­
            if "workflow_log" in node_state and node_state["workflow_log"]:
                latest_log = node_state["workflow_log"][-1]
                log_content += f"{latest_log}\n"
                log_placeholder.text(log_content)

            # 2. ç›´æ¥ç”¨æœ€æ–°çš„çŠ¶æ€å­—å…¸ï¼Œæ¥æ›´æ–°æˆ‘ä»¬çš„æœ€ç»ˆçŠ¶æ€ï¼
            #    åˆ é™¤äº†æ‰€æœ‰ç”»è›‡æ·»è¶³çš„é”™è¯¯ä»£ç ï¼
            final_state.update(node_state)
        # --------------------

        full_response = final_state.get("generation", "å–µï¼Ÿå¥½åƒå‡ºäº†ç‚¹é—®é¢˜...")
        answer_placeholder.markdown(full_response)
        
        detail_info = {
            "standalone_question": final_state.get("standalone_question", "N/A"),
            "documents": final_state.get("documents", []),
            "synthesized_facts": final_state.get("synthesized_facts", "N/A"),
        }
        
        ai_message = AIMessage(content=full_response, additional_kwargs={"detail": detail_info})
        st.session_state.messages.append(ai_message)
        st.rerun()
