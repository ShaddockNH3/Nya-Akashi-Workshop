import os
import re
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
import time
import sys

# --- è£…å¤‡â€œä¸­å¤®å¯¼èˆªç³»ç»Ÿâ€ ---
PROJECT_ROOT = "/root/autodl-tmp/Akashi"
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
try:
    from graph_rag import config
except ImportError:
    print("[è‡´å‘½é”™è¯¯] æ— æ³•ä» 'graph_rag' æ¨¡å—å¯¼å…¥ configï¼")
    exit()

def split_profile_document(content, file_name, is_debug=False):
    """
    ç­–ç•¥ä¸€ï¼šè§’è‰²æ¡£æ¡ˆçš„â€œç²¾å¯†æ‹†è§£â€
    - is_debug=True æ—¶ï¼Œä¼šæ‰“å°å‡ºè¯¦ç»†çš„æå–æ—¥å¿—ã€‚
    """
    # æå–è§’è‰²å
    character_name = file_name.split('_')[-1].replace('.md', '')
    
    # --- ã€é»‘åŒ£å­ã€‘å¯åŠ¨ï¼---
    if is_debug:
        print(f"\n--- [é»‘åŒ£å­] æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name} ---")
        print(f" -> æå–åˆ°çš„è§’è‰²å: '{character_name}'")

    chunks = content.split('\n### ')
    documents = []
    
    for i, chunk in enumerate(chunks[1:]):
        if not chunk.strip():
            continue
            
        full_chunk_content = "### " + chunk.strip()
        section_title = chunk.split('\n')[0].strip()
        
        doc = Document(page_content=full_chunk_content)
        doc.metadata['source'] = file_name
        doc.metadata['category'] = "è§’è‰²äººè®¾"
        doc.metadata['character'] = character_name
        doc.metadata['section'] = section_title
        
        # --- ã€é»‘åŒ£å­ã€‘è®°å½•æ¯ä¸€æ¬¡çš„å…ƒæ•°æ®å†™å…¥ï¼---
        if is_debug:
            print(f"   -> Chunk {i+1} ('{section_title}') -> ç”Ÿæˆçš„å…ƒæ•°æ®: {doc.metadata}")
            
        documents.append(doc)
        
    return documents

# ... (split_story_document å‡½æ•°ä¸å˜)
def split_story_document(content, file_name):
    """
    ç­–ç•¥äºŒï¼šå‰§æƒ…æ–‡æ¡£çš„â€œåœºæ™¯åˆ‡åˆ†â€
    """
    chunks = content.split('\n## ')
    documents = []

    if chunks[0].strip():
        doc = Document(page_content=chunks[0].strip())
        doc.metadata['source'] = file_name
        doc.metadata['category'] = "å‰§æƒ…"
        doc.metadata['scene'] = "åºå¹•"
        documents.append(doc)

    for chunk in chunks[1:]:
        if not chunk.strip():
            continue
            
        full_chunk_content = "## " + chunk.strip()
        scene_title = chunk.split('\n')[0].strip()
        
        doc = Document(page_content=full_chunk_content)
        doc.metadata['source'] = file_name
        doc.metadata['category'] = "å‰§æƒ…"
        doc.metadata['scene'] = scene_title
        documents.append(doc)

    return documents

def load_and_split_documents(knowledge_base_path, is_debug=False):
    print(f"\n[V2.2] [æ­¥éª¤ 1/4] å¼€å§‹åŠ è½½å¹¶è¿›è¡Œâ€œæ™ºèƒ½é¢—ç²’åº¦â€åˆ‡åˆ†...")
    all_docs = []
    profile_folder = "ships"
    
    for folder_name in os.listdir(knowledge_base_path):
        directory = os.path.join(knowledge_base_path, folder_name)
        if not os.path.isdir(directory):
            continue
            
        print(f"  -> æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: '{folder_name}'")
        md_files = [f for f in os.listdir(directory) if f.endswith('.md')]

        for file_name in md_files:
            file_path = os.path.join(directory, file_name)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # --- ä¼ é€’ debug ä¿¡å· ---
                if folder_name == profile_folder:
                    docs = split_profile_document(content, file_name, is_debug)
                else:
                    docs = split_story_document(content, file_name)
                
                all_docs.extend(docs)

            except Exception as e:
                print(f"\n[é”™è¯¯] å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

    print(f"\n[æˆåŠŸ] æ€»å…±åŠ è½½å¹¶åˆ‡åˆ†å‡º {len(all_docs)} ä¸ªçŸ¥è¯†å—ï¼")
    return all_docs

# ... (create_vector_store å‡½æ•°ä¸å˜)
def create_vector_store(docs, save_path, model_name):
    if not docs:
        print("[é”™è¯¯] æ²¡æœ‰çŸ¥è¯†å—å¯ä»¥ç”¨äºå»ºé€ æ•°æ®åº“ã€‚")
        return
    print("\n[æ­¥éª¤ 2/4] å¯åŠ¨ Embedding æ¨¡å‹...")
    model_kwargs = {'device': 'cuda'}
    embeddings_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs
    )
    print("\n[æ­¥éª¤ 3/4] å¼€å§‹å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“...")
    start_time = time.time()
    vector_store = FAISS.from_documents(docs, embeddings_model)
    end_time = time.time()
    print(f"-> æ•°æ®åº“å‘é‡åŒ–å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    data_directory = os.path.dirname(save_path)
    if not os.path.exists(data_directory):
        os.makedirs(data_directory)

    print(f"\n[æ­¥éª¤ 4/4] æ­£åœ¨ä¿å­˜æ•°æ®åº“...")
    vector_store.save_local(save_path)
    print(f"\nğŸ‰ V2.2 æ•°æ®åº“å·²ä¿å­˜åœ¨: '{save_path}'")


if __name__ == "__main__":
    KNOWLEDGE_BASE_PATH = os.path.join(config.PROJECT_BASE_PATH, "clean_knowledge")
    VECTOR_STORE_PATH = config.VECTOR_STORE_PATH
    EMBEDDING_MODEL_PATH = config.EMBEDDING_MODEL_PATH
    
    # --- å¯åŠ¨â€œé»‘åŒ£å­â€æ¨¡å¼ ---
    all_documents = load_and_split_documents(KNOWLEDGE_BASE_PATH, is_debug=True)
    
    if all_documents:
        create_vector_store(all_documents, VECTOR_STORE_PATH, EMBEDDING_MODEL_PATH)

